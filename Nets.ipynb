{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import import_ipynb\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "from functional import  LinearWeightNorm\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim = 28 ** 2, output_dim = 10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            LinearWeightNorm(input_dim, 500),\n",
    "            LinearWeightNorm(500, 500),\n",
    "            LinearWeightNorm(500, 250),\n",
    "            LinearWeightNorm(250, 250),\n",
    "            LinearWeightNorm(250, 250)]\n",
    "        )\n",
    "        self.final = LinearWeightNorm(250, output_dim, weight_scale=1)\n",
    "    def forward(self, x, feature = False, cuda = False, first = False):\n",
    "#        pdb.set_trace()\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        noise = torch.randn(x.size()) * 0.05 if self.training else torch.Tensor([0])\n",
    "        if cuda:\n",
    "            noise = noise.cuda()\n",
    "        x = x + Variable(noise, requires_grad = False)\n",
    "        if first:\n",
    "            return self.layers[0](x)\n",
    "        for i in range(len(self.layers)):\n",
    "            m = self.layers[i]\n",
    "            x_f = F.elu(m(x))\n",
    "            noise = torch.randn(x_f.size()) * 0.5 if self.training else torch.Tensor([0])\n",
    "            if cuda:\n",
    "                noise = noise.cuda()\n",
    "            x = (x_f + Variable(noise, requires_grad = False))\n",
    "        if feature:\n",
    "            return x_f, self.final(x)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, output_dim = 28 ** 2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.fc1 = nn.Linear(z_dim, 500, bias = False)\n",
    "        self.bn1 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
    "        self.fc2 = nn.Linear(500, 500, bias = False)\n",
    "        self.bn2 = nn.BatchNorm1d(500, affine = False, eps=1e-6, momentum = 0.5)\n",
    "        self.fc3 = LinearWeightNorm(500, output_dim, weight_scale = 1)\n",
    "        self.bn1_b = Parameter(torch.zeros(500))\n",
    "        self.bn2_b = Parameter(torch.zeros(500))\n",
    "        nn.init.xavier_uniform(self.fc1.weight)\n",
    "        nn.init.xavier_uniform(self.fc2.weight)\n",
    "    def forward(self, batch_size, cuda = False, seed = -1):\n",
    "        with torch.no_grad():\n",
    "            x = Variable(torch.rand(batch_size, self.z_dim), requires_grad = False)\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        x = F.elu(self.bn1(self.fc1(x)) + self.bn1_b)\n",
    "        x = F.elu(self.bn2(self.fc2(x)) + self.bn2_b)\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
